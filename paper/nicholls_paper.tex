
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{NYC Green Taxis with Neural Network}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Brittany Nicholls}
\IEEEauthorblockA{Department of Computer Science\\ and Electrical Engineering\\
University of Maryland, Baltimore County\\
Baltimore, Maryland 21250\\
Email: brn1@umbc.edu}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
NYC Green TAxi.  Use Neural Net.  Kaggle Competition. RMSE vs RMSLE
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle




% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\section{Hard Lesson Learned}
Dr. Hamed,

I am going to begin this paper by stating a lesson learned the hard way, and unfortunately, learned too late to try to fully rectify the situation (though I'm trying). This project was about trying to predict the travel time for NYC Green Taxis at the beginning of the trip since they have different routes than Yellow Taxis that, I believe, would make them more difficult to model, but a worthwhile exercise.  Also, I wanted to learn how to build a neural net and this seemed like a really good dataset to do that with. :)

Unfortunately, it is the afternoon December 20, 2017 and as I put what I thought would be the finishing touches on the paper (specifically the feature section), I realized that I made a pretty big mistake and I've decided that honesty is the best policy.  To sum it up, I trained my model using information about the dropoff datetime, but the whole basis of my paper was going to be predicting the duration without using that information. Honestly, I think I developed "tunnel vision"  for the past month or so after looking at the Kaggle competition that started me down this road, which includes the dropoff datetime as an available feature \cite{kaggle}. 

 I have currently started another Tensorflow job to try to train a new neural net on the data without the dropoff datetime information; however, it takes about 8-12 hours for the job to finish the 10000 iterations.  I have it saving every 500 iterations, so I will stop the job tonight and do a quick analysis of the iterations it has finished to include in the paper, even if the results are not as good as I hoped.

I sincerely apologize for any inconvenience this causes you.  While I understand there is probably going to be some points taken off for such a blatant mistake in feature generation,  I hope my honesty is appreciated and allows for the focus to be on my efforts instead of on the outcome.

Have a good winter break!  Please let me know if there's anything I should do to fix this issue.

Brittany Nicholls

\section{Introduction}

This paper explores using a neural network in order to predict the duration, in minutes, of a taxi ride for Green Taxis in New York City.  The model is built only using information that would be available at the start of the ride which means that the trip route is not known.

Most people are familiar with the iconic New York Yellow Taxis;  however, there are a second type of taxis in New York City called Green taxis which might also be referenced as Boro taxis.  Green taxis were implemented in 2013 after the government realized that 95\% of Yellow taxi pickups were in central Manhattan, LaGuardia Airport, and JFK Airport \cite{boroBackground}.  You can see a map of the green and yellow zones in Figure \ref{greenZone}.   Note that Green taxis cannot pick up passengers from the yellow zone.

\begin{figure}
  \centering
    \includegraphics[width=\linewidth]{map_service_area.png}
      \caption{Map of NYC with the green taxi pickup areas colored in green. \cite{boroTaxi}}
      \label{greenZone}
\end{figure}

Figure \ref{greenZone} shows us that the rides for Green taxis are very different than the rides of Yellow taxis.  Yellow taxis for the most part stay around Manhattan, while the Green taxis are all over the city.  This means that, for example, the way the drivers find passengers, the traffic the drivers deal with, the routes that drivers take and the number of stoplights the drivers deal with are different between Yellow taxis and Green taxis.  In addition, the number of trips that Green taxis and Yellow taxis make in a day are very different.  Doing a basic comparison between the 2016 taxi datasets available at NYC Open Data, we see that there are 16.4 million records for Green taxis from January 1, 2016 to June 30, 2016 \cite{green2016} while there are 133 million records for Yellow taxis for the whole year of 2016 \cite{yellow2016}.  

There has been previous work on these datasets which has focused on both the datasets for Yellow and Green taxis; however, given the obvious differences in taxi trip details as well as data size, this report documents my attempt to model trip duration for only the Green taxi data.  The best results were achieved using a basic feed-forward neural net to minmize the root mean squared error (RMSE) of the rounded trip duration in minutes; however, these results did use


\subsection{Motivation}
Originally, I was going to try to work on the problem of taxi trip duration prediction introduced in a Kaggle challenge \cite{kaggle}.  However, as I explored existing work surrounding this problem, I realized that when most papers look at the NYC taxi data, they either combine Yellow and Green taxi data \cite{blog}, or only focus on Yellow taxis \cite{kaggle}\cite{ucsd}.  Not to mention, most work available has been done by students or through Kaggle.

Predicting taxi trip duration is important for multiple reasons.  First, trip duration can be used by a customer to schedule their ride or to estimate the fare for a trip.  Second, it could allow taxi companies to start developing, or improving, their ability to chain together rides to increase profit for a driver.  Lastly, having a baseline for what the expected trip duration would allow taxi companies to detect when drivers are taking their passengers on the "scenic route" in order to charge the passengers more.  Note that in these cases, having an exact time prediction in seconds is not necessary as a general "it will take x minutes" will suffice.  This is why the model is going to train on the duration of the trip in rounded minutes and not seconds.


\section{Related Work}
As I mentioned above, the work on this dataset has been done informally through school projects or Kaggle competitions; however, there is other work related to taxi data.

\subsection{Existing Work on NYC Taxi Trip Duration Prediction}
In July 2017, Kaggle issued a challenge to Kagglers to work on a pre-selected NYC taxi trip dataset in order to predict the trip duration; however, the focus of the competition was to encourage collaboration, so the top publically available kernels are focused on exploring the data in a clear way to benefit the group \cite{kaggle}.  In addition, the few publically available codebases were not from top performers \cite{currie} \cite{yukw} \cite{mk}.  Since it was still a competition to predict the trip duration, Kaggle still provided the means for evaluating the models using RMSLE.  After evaluating using RMSLE loss function, I decided to go with RMSE as my loss function instead.  I will go into more detail in section \ref{loss}.

Additionally, other students have used this dataset to predict trip duration and made their work publically available; however, none of them used neural nets, which at the moment are a "hot" thing to do because of their ability to fit to a lot of datasets fairly well.  In one paper, they used linear regression and random forests in order to build their model to predict the number of minutes the trip will take and achieved a low RMSE of 5.24 \cite{stanford}.  Another project achieved an RMSE of 4.87 by using a gradient boosting regressor \cite{ucsd}.  I used the second paper that achieved a RMSE of 4.87 as a baseline, which will be discussed in section \ref{baseline}. 

\subsection{Other Work on Taxi Data}
Ferreira et. al. have also used the NYC Taxi data in order to discuss how to best visualize and query datasets such as the NYC taxi data \cite{query}.  While their work is interesting and does use the NYC taxi data, it does not attempt to make any predictions on the trip duration.

It should be noted that in 2015 there was another Kaggle competition to predict the trip duration for taxi data from Porto, Portugal \cite{kagglep}.  One of the top ranking results were published by a group from IBM in which they discuss how they would predict the final destination based on the beginning trip trajectory and would use that to predict the trip duration \cite{ibm}.  However, they were predicting the trip duration based on the initial route of the taxi after picking up a customer, which contains very different data than is available for the NYC taxi data.

\subsection{Other Work on Trip Duration Prediction}
Work has been done to predict when a bus will arrive at its stop using an algorithm based on Kalman Filtering  \cite{india}; however, their work was focused on data collected in India and they needed to be able to analyze the data real time.  First, traffic in India is probably different from traffic in NYC.  Second, bus routes and stops are well defined while taxi routes and start/end locations are not pre-determined until a customer gets into a taxi. 

More work on bus arrival prediction has been done by Biagoni et. al. in which they use a smartphone that is placed on a bus to predict when the bus will arrive at its next stop \cite{et}.  Once again, this is bus data and relies on real time data for a specific route with specific stops.

Additionally, there has been quite a bit of work on predicting travel time on freeways \cite{freewayca} \cite{highway}.  However, driving in a city is very different from driving on a freeway.


\section{Data and Data Challenges}
 

\subsection{NYC Open Data}
New York City made the data for January 2016 - June 2016 of the trip records for the Green taxi data publically available \cite{green2016}. There are approximately 16 million rows.  This data contains the following information:
\begin{itemize}
\item{Vendor ID : One of two vendors}
\item{Pickup and Dropoff Date time: Time and Date when customer was picked up and dropped off}
\item{RateCode ID: Rate (and the code) change based on where the customer is going}
\item{Trip Type: Was the taxi hailed or dispatched}
\item{Pickup and Dropoff Location: Contains the lat/long for both of these locations}
\item{Passenger count: Driver recorded number of passengers in the vehicle for the ride}
\item{Information about the fare: Base rate, taxes, tips}
\end{itemize}

I enriched this dataset with two other datasets: weather and sunrise/sunset times.

\subsection{Weather}
Since traffic and taxi use can be heavily influenced by weather, I pulled down some basic stats about the weather for the taxi data being analyzed. The weather for April and March was retrieved from a site run by NOAA \cite{weather}.  There was one record per day containing:
 \begin{itemize}
\item{Minimum/Maximum/Average temperature}
\item{Precipitation: The amount of rain that fell.  Note that a value of T stands for trace}
\item{New Snow: The amount of new snow that fell}
\item{Snow depth: How deep the snow was}
\end{itemize}
Note that both the New Snow and Precipitation columns contained a few values of "T", which stands for trace.  This means that very little snow/rain fell and it was not able to be measured.  Records that had a value of "T" were assigned the value of 0.00001 based on a recommendation in the Journal of Service Climatology \cite{trace}.

\subsection{Sunrise/Sunset}
Lastly, the taxi data was enriched with sunrise and sunset information.  Note that the sunrise and sunset can affect traffic because the sun may get into people's eyes, causing them to slow down.  The sunset and sunrise times were retrieved from a site owned by the US Navy \cite{sun}.


\section{Data Cleaning and Features Used}
The Green Taxi data used for training and testing was cleaned and filtered.  Mostly, the data needed to be filtered because there was so much of it and the limited resources on the computer being used to build the model would not have supported training a neural net on millions of datapoints.  Instead, about a million points were used for training, 100k were used for validation, and 300k were used for testing.  All of the training/validation/testing data came from April of 2016. In addition, the set of filtered data  for March of 2016 was also extracted to see how well the model might transfer to a different month of taxi rides.

One thing to be aware of is the definition of the Vincenty distance.  Basically, the Vincenty distance is an approximation of the distance between two geo points "as the bird flies".  Note that this distance isn't exact, but it is fairly close.  This distance can be calculated at the beginning of the trip using the geo points, so it is one of the features of this dataset.

Some of the steps taken to filter the data were:
\begin{enumerate}
\item{Remove bad geo points: Some of the pickup and dropoff lat/longs were null, one of the points would be 0, or the points would be in areas outside the area of interest (NYC).  In fact, a polygon was loosely drawn around the area shown in Figure \ref{greenZone} and the geo for the pickup and dropoff points had to be in that polygon. }
\item{Rate Codes: The data was filtered for either negotiated rates or the standard rates.  Since there was a small percentage of trips to LaGuardia or JFK, these were removed, as were records with an "unknown/bad" rate code.}
\item{NonZero Trip Distance: The original dataset provided the trip distance as recorded by the odometer.  If this recorded trip distance was 0, the trip was removed }
\item{Trip Distance < Vincenty Distance - 0.1: If the distance the odometer recorded is less than the Vincenty distance by more than 0.1 miles, the record was removed.}
\item{Low Trip Cost/High Fare per Miles: Records that had low low trip cost or high fare per miles were determined to be bad data and removed.  On one hand, drivers are not going to take a trip that will cost them money and the threshhold set was \$0.50/mile, but most riders are not going to pay a lot of money to go a small distance, which threshhold was determined to be \$50/0.1 mile - about \$50 a block.}
\item{Very Long/Short Durations: While it is possible that a taxi might only take a passenger for a trip of less than 30 seconds or that a customer might rent a taxi for more than 3 hours, these were considered outliers and records were removed.  3 hours was the upper end because traffic can significantly lengthen a trip.}

\end{enumerate}

In essence, the datasets are for green taxi trips that start and end in New York City (not including the airport).  The data was filtered to try to catch primarily one way rides that are a reasonable cost/distance/duration for the trip itself, the driver and the passenger.

After filtering the data as described above, some features were generated based off of the assumption that the information would have to exist at the beginning of the trip.  
In the main model being discussed, the following features were used:
 \begin{itemize}
\item{Pickup/Dropoff Location: Lat/Long for pickup and dropoff location}
\item{Minute of Day for Pickup/Dropoff: The minute of the day that the pickup and dropoff was made - This is the mistake that I mention in the beginning}
\item{Day of the week for Pickup/Dropoff: A one-hot encoded value representing each day of the week}
\item{Passenger Count: How many passengers were in the vehicle}
\item{Vincenty Distance in miles}
\item{Weather: Average temperature, amount of rainfall, amount of snowfall}
\item{Inverse of the number of minutes after Sunrise/until Sunset: Only for a 1.5 hour window after/before sunrise/sunset. 0 for the rest of the times.}
\item{Rate Code ID, Trip Type, Vendor ID:One hot encoding was performed for these features.}
\end{itemize}

\section {Baseline Model} \label{baseline}
The baseline used was proposed by Jaiwal et. al. \cite{ucsd}.  Basically, their best-performing approach was to use k-means to create 40 clusters of the geo points for the pickup/dropoff locations and then train a Gradient Boosting Regressor on a similar feature set to mine \cite{ucsd}.  They also used RMSE to calculate their error and ended up achieving an RMSE of 4.87 for duration in minutes.

Because they worked over a different, but related dataset, I decided to try to implement their algorithm over my data.  Unfortunately, they only had a significantly smaller training dataset and when trying to run sklearn's Gradient Boosting Regressor over the green taxi training dataset, the computer ran out of memory.  Therefore, a smaller sample of the training dataset (50k samples) was used and after performing their algorithm over the smaller dataset, the calculated RMSE was 5.66.

Once again, I realized that my mistake in including dropoff time information in the feature set that was run through this algorithm probably affected the results.  Given this realization, the calculated RMSE for this baseline can only be used for the neural network model that also used this information.  However, I cannot compare the neural network model I attempted to train without the dropoff datetime features against this model.

\section{Predictive Model}


\subsection{Loss Function} \label{loss}
I debated between using two loss functions, RMSLE and RMSE.  Utlimately, I choose to use RMSE and will explore why in this section.

Kaggle decided to use RMSLE \cite{kaggle}:

$\epsilon = \sqrt{\frac{1}{n} \sum{(log(p_i + 1) - log(a_i + 1))^2}}$

with a little bit of manipulation of the logs, this transforms into:

$\epsilon = \sqrt{\frac{1}{n} \sum{log(\frac{p_i + 1}{a_i + 1})^2}}$

This shows that, in essence, the model is heavily punished when $p_i < a_i$ as compared to when $p_i > a_i$, even if the difference is the same.  This means that this model will try to avoid predicting under the actual value, since the loss isn't as severe if it goes over.  Thus, minimizing the RMSLE loss is better for finding a model that will predict the minimum trip duration.  Furthermore, because of it's use of the log function, RMSLE would be good for numbers that are going to be large.  For instance, if we were to predict the trip duration in milliseconds or seconds (as the Kaggle competition did), then RMSLE might be a better choice; however, I choose to focus on predicting the trip duration in minutes, not seconds.

RMSE is calculated as follows:

$\epsilon = \sqrt{\frac{1}{n} \sum{(p_i - a_i)^2}}$

which shows that the model will get equally punished for predicting too large of a value as it will too small of a value.  Thus, I choose to minimize the RMSE when building my model.

\subsection{Neural Network Settings for Best Results}
Utimately, the best results came from a feed-forward neural network with two hidden layers.  38 features were fed into the network and each hidden layer had 40 neurons.  Note that these were the features discussed above.  The activation function used for each neuron was the rectified linear unit.  Instead of using the plain gradient descent optimizer available through Tensorflow, the Adam Optimizer was experimented with because of the larger number of features \ref{adam}.  

Up to 9500 steps were taken, with each step training on 300 randomly selected datapoints.  Every 500 steps, the model was saved off; however, Tensorflow only retains the 5 newest models, so the data was validated on the model after 7500, 8000, 8500, 9000, and 9500 iterations.

At the end, the validation data was used to determine which iteration would be the best model.  Figure \ref{ohAcc} shows that, interestingly enough, the model had the lowest error after 7500 iterations and was very closely followed by the model after 9500 iterations; however, there is a spike at 8500 iterations in which the error doubles.  This probably indicates that the model had found a local minima after 7500 steps, but was moved out of it during the following steps.  Thus, it was decided  to use the model at 9500 to evaluate the RMSE of the test data.  The RMSE of the test data for this model is 2.12, which is less than half of the baseline RMSE for this data.

 \begin{figure}
  \centering
    \includegraphics[width=\linewidth]{reluOneHotAdam.png}
      \caption{Validation error (RMSE) for the best performing neural network}
      \label{ohAcc}
\end{figure}

The jump in error at 8000 iterations for the validation data could be due to the algorithm stepping out of a local minima to the function, but it is possible that the model was starting to overfit.  The fact that the validation error became lower as more steps were taken points to the fact that this may not be the case.  

Technically, the validation data came from data that might be very similar to the training data, so it may not have picked up the possibility of overfitting.  For curiousity's sake, the graph of the iterations vs the test error has been provided in Figure \ref{ohTest}.

 \begin{figure}
  \centering
    \includegraphics[width=\linewidth]{reluOneHotAdamTestData.png}
      \caption{Test error (RMSE) for the different iterations of best performing neural network}
      \label{ohAcc}
\end{figure}

This indicates that the data is very likely not being overfit since the model didn't see any data that shares the same dates as the testing data.  Furthermore, in section \ref{march}, we explore how well this model transfers to the green taxi data, but for March instead of April.

\subsubsection{Visualizing Results}
While it's great that the RMSE was minimized and is half of that of the baseline model, it is important to make sure that the actual results look right.  Figure \ref{predAct} visualizes the actual trip time (in rounded minutes) against the predicted trip time (in rounded minutes - done after the raw prediction).  The red line in the figure represents the correct predictions.  If predicted values are fairly close to the actual values, the points should be grouped around the red line, and as you can see, while not perfect, most of the predictions are near the red line.


 \begin{figure}
  \centering
    \includegraphics[width=\linewidth]{actualVsPredicted.png}
      \caption{Scatterplot of the actual durations vs the predicted durations}
      \label{predAct}
\end{figure}

\subsection{Model Against March Data} \label{march}
In order to see how robust the model was, we tried the model built using the training data from April against the March dataset.  The RMSE of the March data against this model is 2.24, which was significantly lower than might be expected.

\subsection{Alternative Settings Considered}
Unfortunately, it is difficult to perform proper cross validation on a neural net, especially given the shorter timeframe and limited computing resources.  Each neural net took 8-12 hours to train with 10000 iterations.  I had tried training the neural net using tanh as the activation function, but the loss on the training data for that was poor enough that I switched to relu, which had much better/slower loss results.  We tried training against the non one-hot encoded data to see if the neural net could accomodate the more complicated features, but the RMSE for that was above 10, which was even worse than the baseline; that neural net also used the basic gradient descent optimizer and each hidden layer only had 25 neurons, which both may have been a cause for the higher error.

\subsubsection{RMSLE}
I did spend some time trying to build a neural net for the Kaggle competition in the beginning to familiarize myself with the data and Tensorflow.  I was actually able to achieve a RMSLE of 0.29184, which is fantastic given that the top ranked score was 0.28976, with second place achieving 0.30664.  Unfortunately, we cannot directly compare the two scores because 1) I did not use the Kaggle data and 2) there is a possibility my data was a lot cleaner than the Kaggle datasets; however, this was a strong indication that building a neural net using these features was the route to go.  Additionally, Kaggle did allow the model to have access to the dropoff datetime during training and testing, so fortunately, this work wasn't affected by that discovery.

Figure \ref{predActRMSLE} shows the actual values vs the predicted values.

 \begin{figure}
  \centering
    \includegraphics[width=\linewidth]{actualVsPredictedRMSLE.png}
      \caption{Scatterplot of the actual duration (in seconds) vs the predicted durations (in seconds) for the model minimizing RMSLE}
      \label{predActRMSLE}
\end{figure}



\subsection{Predictive Model without Dropoff Time Information}
As I mentioned above, it completely slipped my notice that I was training my models on training data that had the pickup time in it.  In order to try to make up for this, I tried to run the model discussed above over the same feature set but without the dropoff time related information.  The model had to be stopped early for time reasons.  Unfortunately, the RMSE for both the validation and test sets were less than ideal.  The lowest RMSE of the validation set was 6.55 at iteration 6500 and the RMSE of the test data for the same model was 6.31.  These numbers might have minorly improved had there been more steps taken, but most likely not enough to say that this model would beat the baseline.

In great news, this does prove that the neural net above was trained to heavily use the dropoff datetime related features.  This is good, in a sense, because knowing the dropoff time does give you a distinct advantage and the neural net should have designed a function around relying on that data as it would be the most accurate way to predict the duration of the trip.



\section{Future Work and Ideas}
Ideally, the future work would have included ideas to potentially improve upon the model; however, after realizing that I had been training on data with the answer, in essence, in the feature set I would need to continue to improve upon this model.  Ultimately, I would focus on constructing the neural net differently by changing the activation functions as well as the number of neurons in the hidden layers.  I would also explore having different activation functions on different layers.  Lastly, I would see if using a third hidden layer would improve the model.

In all honesty, I was expecting to spend a bit of time today to explore doing some sort of ensembling between the RMSLE predictions in seconds and the RMSE predictions in minutes to estimate the minutes.  If you look at Figure \ref{predActRMSLE}, it seems that this approach was good for predicting trip duration for shorter trips; however, it was not good at predicting longer trips.  Figure \ref{predAct} shows that that approach could better predict the longer trips, although it's hard to see how well it worked on the shorter trips due to the cloud around the shorter trips.  One of the ensembling methods I was going to look at was going to be using a gradient boosting regressor; however, I was also going to try just taking the RMSLE prediction in seconds, converting it to minutes, and taking the rounded average between that and the RMSE prediction that was doing so well.

Some other ideas for after creating a good model were to introduce small geoboxes covering NYC to allow for some geo error.  This is similar to what the clustering did in the baseline algorithm; however, the clustering is going to be affected by the pickup and dropoff locations.  In addition, I would explore using a neural net to predict the trip distance, as this was data that was filtered out.  The predicted distance could potentially be fed into the neural net that predicts the duration, instead of using the Vincenty distance.  Lastly, there has been work on predicting when and where taxis are going to be needed based on zones \cite{blog}.  This information could be used in order to predict where taxis are going to be needed which can, in a way, predict traffic.

\section{Conclusion}
Unfortunately, neural nets were not able to meet the goal of this report of predicting NYC green taxi trip durations based off of information available at the start of a taxi ride; however, it is important to note that this is because I made a mistake and did not realize early enough that the neural nets were being trained on data that had the features related to the dropoff datetime information.  I made a beginners error, and I made it early enough that it was propogated through all aspects of this project starting with the baseline.  

It is still valuable that the work that has been done is presented because 1) it exhibits the very painful lesson I have learned of needing to triple check the reason for including every feature, 2) the model that I did create could be used after a taxi trip as a potential baseline in which to try to predict anomalous taxi trips against and 3) it does show that neural nets will use the best features to their utmost advantage.  In this case, I had used the pickup and dropoff time (minute in the day) to predict the duration.  Any function trying to do this prediction should very definitely try to use as much information as it can from the pickup and dropoff times.  As we saw, when the neural net lost the information about the dropoff times, the RMSE increased because the predictions were worse.  While it is not good that the neural net without the the dropoff time features performed poorly, that is very likely due to not be configured properly.  Based off of this report, I wouldn't stop trying to use neural nets in predicting taxi times, instead, I would focus on tuning the neural net for the correct feature set.


% conference papers do not normally have an appendix


% use section* for acknowledgment





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,taxi}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

%\bibliography{baltimoreBib}
%\bibstyle{filename}
%\nocite{*}
%\begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}




% that's all folks
\end{document}

